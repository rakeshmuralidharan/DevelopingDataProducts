lmF <- lm(Numeric ~ Year, data = hunger[,hunger$Sex == "Female"])
lmF <- lm(Numeric ~ Year, data = hunger[hunger$Sex == "Female",])
lmF <- lm(Muneric[Sex == "Female"] ~ Year[Sex == "Female"], hunger)
lmF <- lm(Numeric[Sex == "Female"] ~ Year[Sex == "Female"], hunger)
lmM <- lm(NUmeric[Sex == "Male"] ~ Year[Sex == "Male"], hunger)
lmM <- lm(Numeric[Sex == "Male"] ~ Year[Sex == "Male"], hunger)
lmBoth <- lm(Numeric ~ Year + Sex, hunger)
summary(lmBoth)
lmInter <- lm(Numeric ~ Year + Sex + Sex*Year, hunger)
summary(lmInter)
fit <- lm(y ~ x, out2)
plot(fit, which = 1)
fitno <- lm(y ~ x, out[-1,])
fitno <- lm(y ~ x, out2[-1,])
plot(fitno, which=1)
coef(fit) - coef(fitno)
head(dfbeta(fit))
resno <- out2[1, "y"] - predict(fitno, out2[1,])
1 - resid(fit)[1]/resno
head(hatvalues(fit))
sigma <- (sum(summary(fit)$resid)^2)/summary(fit)$df
sigma <- sqrt(deviance(fit)/df.residual(fit))
rstd <-
summary(fit)$resid/(sigma*sqrt(1-hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which=3)
plot(fit, which=2)
sigma1 <-
)
sigma1 <- sqrt(deviance(fitno)/df.residuals(fitno))
sigma1 <- sqrt(deviance(fitno)/df.residual(fitno))
resid(fit)[1]/(sigma1 * sqrt(1-hatvalues(fit)[1]))
head(rstudent(fit[1]))
head(rstudent(fit)[1])
head(rstudent(fit))
predict(fitno, out2) - predict(fit, out2)
dy <- predict(fitno, out2) - predict(fit, out2)
sum(dy^2)/(2*sigma^2)
plot(fit, which=5)
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality, swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, swiss)
vif(mdl2)
x1c <- simbias()
apply(x1c, 1, mean)
fit1 <- lm(Fertility ~ Agriculture, swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
featureplot(x = training[, -CompressiveStrength], y=training$CompressiveStrength, plot=pairs)
featurePlot(x = training[, -CompressiveStrength], y=training$CompressiveStrength, plot=pairs)
head(training)
featurePlot(x = training, y=training$CompressiveStrength, plot=pairs)
featurePlot(x = training[, c(1:8)], y=training$CompressiveStrength, plot=pairs)
featurePlot(x = training[, training$CompressiveStrength], y=training$CompressiveStrength, plot=pairs)
featurePlot(x = training$CompressiveStrength, y=training$CompressiveStrength, plot=pairs)
featurePlot(x = training$CompressiveStrength, y=training$CompressiveStrength, plot=pairs)
library(caret)
?featurePlot
str(training)
training$cutCement = cut2(training$Cement, g=3)
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
str(training)
training$cutFlyAsh = cut2(training$FlyAsh, g=3)
training$cutAge = cut2(training$Age, g = 3)
featurePlot(x = training[, c(cutFlyAsh, cutAge), y=training$CompressiveStrength, plot=pairs)
featurePlot(x = training[, c(cutFlyAsh, cutAge)], y=training$CompressiveStrength, plot=pairs)
str(training)
featurePlot(x = training[, c(training$cutFlyAsh, training$cutAge)], y=training$CompressiveStrength, plot=pairs)
library(qplot)
install.packages("ggplot")
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
library(ggplot)
library(ggplot2)
?qplot
qplot(cutAge, CompressiveStrength, data=training)
featurePlot(x = training[, c(training$cutFlyAsh, training$cutAge)], y=training$CompressiveStrength, plot=pairs)
install.packages
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
hist(training$Superplasticizer)
table(training$superplasticizer)
head(training[Superplasticizer == 0,])
head(training[training$Superplasticizer == 0,])
log10(training[training$Superplasticizer == 0,])
min(training[training$Superplasticizer == 0,])
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
?preProcess
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
subset = training[, c(IL_11 28 1.27 1.27 1.29 1.27 ...
$ IL_16                           : num  2.67 3.48 2.15 3.59 2.88 ...
$ IL_17E                          : num  3.64 3.64 4.75 3.87 5.73 ...
$ IL_1alpha                       : num  -8.18 -7.37 -7.85 -8.05 -7.85 ...
$ IL_3                            : num  -3.86 -4.02 -4.51 -3.58 -4.51 ...
$ IL_4                            : num  1.21 1.81 1.57 1.92 1.81 ...
$ IL_5                            : num  -0.4 0.182 0.182 0.336 0 ...
$ IL_6                            : num  0.186 -1.534 -1.097 -0.399 0.422 ...
$ IL_6_Receptor                   : num  -0.5173 0.0967 0.354 0.0967 -0.5322 ...
$ IL_7                            : num  2.78 2.15 2.92 2.92 1.56 ...
$ IL_8
)]
subset = training[, IL_11, IL_13, IL_16, IL_17E, IL_1alpha, IL_3, IL_4, IL_5, IL_6, IL_6_Receptor, IL_7, IL_8]
subset = training[, training$IL_11, training$IL_13, training$IL_16, training$IL_17E, training$IL_1alpha, training$IL_3, training$IL_4, training$IL_5, training$IL_6, training$IL_6_Receptor, training$IL_7, training$IL_8]
subset = training[, training$IL_11, training$IL_13, training$IL_16, training$IL_17E, training$IL_1alpha, training$IL_3, training$IL_4, training$IL_5, training$IL_6, training$IL_6_Receptor, training$IL_7, training$IL_8]
str(training)
library(ElemStatLearn)
data(prostate)
str(prostate)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
str(vowel.train)
str(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
str(vowel.train)
vowel.test$y = as.factor(vowel.test$y)
str(vowel.test)
library(caret)
set.seed(33833)
rfFit = train(y~., data=vowel.train, prox = TRUE)
summary(rfFit)
rfFit
rfPred = predict(rfFit,vowel.test)
rfPred
summary(rfPred)
table(pred,vowel.test$y)
table(rfPred,vowel.test$y)
nrows(vowel.test)
nrow(vowel.test)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
install.package("gbm")
install.packages("gbm")
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
set.seed(62433)
rfmod = train(diagnosis~ ., method="rf", data=training, trControl=trainControl(method="cv"), number=3)
rfmod
gbmmod = train(diagnosis ~., method = "gbm", data=training)
ls()
ldamod = train(diagnosis~., method="lda", data=training)
ldsmod
ldamod
gbmmod
rfpred = predict(rfmod, testing)
rfpred
table(rfpred, testing$diagnosis)
nrow(testing)
rfpred = predict(gbmmod, testing)
gbmpred = predict(gbmmod, testing)
gbmpred
table(gbmpred, testing)
table(gbmpred, testing$diagnosis)
ldapred = predict(ldamod, testing)
ldapred
table(ldapred, testing$diagnosis)
rfpred = predict(rfmod, testing)
table(rfpred, testing$diagnosis)
qplot(rfpred, gbmpred, ldapred, color=diagnosis, data=testing)
qplot(rfpred, gbmpred, color=diagnosis, data=testing)
predDF = data.frame(rfpred, gbmpred, ldapred, testing$diagnosis)
head(predDF)
combmod = train(diagnosis~., method = "rf", data=predDF)
predDF = data.frame(rfpred, gbmpred, ldapred, diagnosis = testing$diagnosis)
head(predDF)
combmod = train(diagnosis~., method = "rf", data=predDF)
combmod = train(diagnosis~., method = "rf", data=predDF, prox=TRUE)
combmod
combpred = predict(combmod, predDF)
table(combpred, predDF$diagnosis)
combmod = train(diagnosis~., method = "rf", data=predDF)
combpred = predict(combmod, testing)
table(combpred, testing$diagnosis)
?lasso
??lasso
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
str(training)
lassomod = train(CompressiveStrength ~ ., method="lasso", data=training)
lassomod
lassopred = predict(lassomod, testing)
lassopred
?plot.enet
plot(lassomod, xvar="step")
plot(lassomod)
?plot.enet
plot(lassomod, xvar="penalty")
set.seed(233)
lassomod = train(CompressiveStrength ~. method = "lasso", data=training)
lassomod = train(CompressiveStrength ~., method = "lasso", data=training)
lassopred = predict(lassomod, testing)
plot(lassomod, xvar="penalty")
plot.enet(lassomod$finalModel, xvar = "penalty", use.color = TRUE)
getwd()
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(lubridate)  # For year() function below
dat = read.csv("~/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
getwd()
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
getwd()
dat = read.csv("C:/Program Files/RStudio/R2/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
str(training)
library(forecast)
library(quantmod)
install.packages("forecast")
install.packages("quantmod")
library(forecast)
library(quantmod)
fit <- bats(tstrain)
h <- dim(testing)[1]
fcast <- forecast(fit, level = 95, h = h)
accuracy(fcast, testing$visitsTumblr)
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
library(caret)
fit <- train(CompressiveStrength ~ ., data = training, method = "svmRadial")
prediction <- predict(fit, testing)
accuracy(prediction, testing$CompressiveStrength)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
fit3 <- train(diagnosis ~ ., data = training, method = "lda")
predict1 <- predict(fit1, newdata = testing)
predict2 <- predict(fit2, newdata = testing)
predict3 <- predict(fit3, newdata = testing)
DF_combined <- data.frame(predict1, predict2, predict3, diagnosis = testing$diagnosis) # training$diagnosis?
fit_combined <- train(diagnosis ~ ., data = DF_combined, method = "rf")
predict4 <- predict(fit_combined, newdata = testing)
c1 <- confusionMatrix(predict1, testing$diagnosis)
c2 <- confusionMatrix(predict2, testing$diagnosis)
c3 <- confusionMatrix(predict3, testing$diagnosis)
c4 <- confusionMatrix(predict4, testing$diagnosis)
print(paste(c1$overall[1], c2$overall[1], c3$overall[1], c4$overall[1]))
require(rCharts)
install.packages("rCharts")
library(rCharts)
ap = available.packages()
head(ap)
View(ap) "rCharts" %in% rownames(ap)
library(googleVis)
install.packages("googleVis")
library(googleVis)
M = gvisMotionCharts(Fruits, "Fruit", "Year", options=list(width=600, height=400))
M = gvisMotionChart(Fruits, "Fruit", "Year", options=list(width=600, height=400))
print(M, "chart")
print(M, "chart")
library(googleVis)
M = gvisMotionChart(Fruits, "Fruit", "Year", options=list(width=600, height=400))
print(M, "chart")
source('~/.active-rstudio-document', echo=TRUE)
library(googleVis)
M = gvisMotionChart(Fruits, "Fruit", "Year", options=list(width=600, height=400))
print(M, "chart")
demo(googleVis)
plot(M)
source('~/.active-rstudio-document', echo=TRUE)
G = gvisGeoChart(Exports, locationvar="Country", colorvar="Profit", options=list(width=600, height=400))
plot(G)
G1 = gvisGeoChart(Exports, locationvar="Country", colorvar="Profit", options=list(width=600, height=400, region=150))
plot(G1)
install.packages("devTools")
install.packages("devtools")
library(devtools)
library(plotly)
install.packages("plotly")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(caret)
data(segmentationData)
str(segmentationData)
segmentationData$Cell = NULL
str(segmentationData)
training = subset(segmentationData, Case == "Train")
str(training)
testing = subset(segmentationData, Case == "Test")
str(testing)
training$Case = NULL
testing$Case = NULL
str(training[,1:9])
cell_lev = levels(testing$Class)
cell_lev
trainX = training[, names(training) != "Class"]
preProcvalues = preProcess(trainX, method = c("center", "scale"))
preProcValues
str(preProcValues)
library(caret)
preProcValues = preProcess(trainX, method=c("center", "scale"))
preProcValues
scaledTrain = predict(preProcValues, trainX)
library(rPart)
library(rpart)
rpart1 = rpart(Class ~., data=training, control=rpart.control(maxdepth=2))
rpart1
rpart.plot(rpart1)
plot.rpart(rpart1)
rpart1a = as.party(rpart1)
library(partykit)
install.packages(partykit)
rpartFull = rpart(Class~., training)
rpartFull
rpartPred = predict(rpartFull, testing)
confusionMatrix(rpartPred, rpartFull$Class)
confusionMatrix(rpartPred, testing$Class)
rpartPred = predict(rpartFull, testing, type="class")
confusionMatrix(rpartPred, testing$Class)
rpartTune = train(Class ~ ., data=training, method="rpart")
rpartTune = train(Class ~ ., data=training, method="rpart", tuneLength = 10)
cvCtrl = trainControl(method = "repeatedcv", repeats = 5)
rpartTune = train(Class ~ ., data=training, method="rpart", tuneLength = 10, trControl = cvCtrl)
ls()
fakeData = data.frame(pred=testing$Class, obs=sample(testing$Class), PS = runif(nrow(testing)))
str(fakeData)
twoClassSummary(fakeData, lev = levels(fakeData$obs))
library(CART)
install.packages("CART")
library(caret)
twoClassSummary(fakeData, lev = levels(fakeData$obs))
install.packages("pROC")
library(pROC)
twoClassSummary(fakeData, lev = levels(fakeData$obs))
twoClassSummary(fakeData, lev = levels(fakeData$obs))
cvCtrl = trainControl(method = "repeatedcv", repeats = 5, summaryFunction = twoClassSummary, classProbs = TRUE)
set.seed(1)
rpartTune = train(Class ~ ., data=training, method = "rpart", tuneLength = 10, metric = "ROC", trControl = cvCtrl )
rpartTune = train(Class ~ ., data=training, method = "rpart", tuneLength = 10, metric = "ROC", trControl = cvCtrl )
str(training)
library(pROC)
?ROC
??ROC
Developing Data Products Project - Exploration and prediction of crime rates.
Exploration and prediction of crime rates.
file.edit('~/.Rprofile')
getwd()
ls
ls()
file.edit('~/.Rprofile')
options(rpubs.upload.method = "internal")
Exploration and prediction of crime rates.
Exploration and prediction of crime rates.
test
Exploration and prediction of crime rates.
Exploration and prediction of crime rates.
========================================================
author: Rakesh Muralidharan
date: 24-May-2015
Description:
========================================================
- This is a reproducible pitch to the project done with Shiny for Developing Data Products course.
getwd()
setwd("C:/Users/juliuscezar/Desktop/Coursera/Shiny/App-1")
Exploration and prediction of crime rates.
========================================================
author: Rakesh Muralidharan
date: 24-May-2015
Description:
Exploration and prediction of crime rates.
========================================================
author: Rakesh Muralidharan
date: 24-May-2015
Description:
========================================================
- This is a reproducible pitch to the project done with Shiny for Developing Data Products course.
- The project accesses the "USArrests" data set which measures violent crime rates (per 100,000) for US states by percent of urban population.
- The Shiny application can be accessed through the following hyperlink on ShinyServer
https://rakeshmuralidharan.shinyapps.io/App-1/
Instructions:
========================================================
- Following are the instructions for using the Shiny app mentioned in the previous slide.
- From the first sidebar drop down menu, select the crime type - Murder, Assault or Rape. This will reactively plot the selected crime rate vs. urban population percent (default is Murder).
- Next from the second slidebar option select the value of urban population for which you want the model to predict the crime rate. This will run a simple regression of selected crime rate with urban population as independent variable and print out the predicted crime rate.
- Further down the page you will see the ui.R and server.R files and some descriptions. Enjoy!
Sample plot found on Shiny application
========================================================
- Here is a sample plot for Assault vs. Urban Population with a regression line.
```{r, echo=FALSE}
library(ggplot2)
library(datasets)
p = ggplot(data=USArrests, aes_string(x="UrbanPop", y="Assault"))+geom_point(color="red", size=4) + geom_smooth(method=lm, se=FALSE)
print(p)
```
Sample regression run on the Shiny application
========================================================
- Here is a sample regression run on the Shiny app. This one regresses Assault with urban population percent.
```{r}
regfit = lm(Assault ~ UrbanPop, data=USArrests)
coef(regfit)
```
source('~/.active-rstudio-document', echo=TRUE)
```
ProjectPresentation
options(rpubs.upload.method = "internal")
ProjectPresentation
getwd()
Exploration and prediction of crime rates.
